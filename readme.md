
<p align="right">
<a href="https://autorelease.general.dmz.palantir.tech/palantir/spark-tpcds-benchmark"><img src="https://img.shields.io/badge/Perform%20an-Autorelease-success.svg" alt="Autorelease"></a>
</p>

Spark Benchmark Runner
======================

This repo contains tools to run 2 industry standard benchmark suites:

1. [TPC-DS](http://www.tpc.org/tpcds/) is an open benchmark suite for structured data systems. This utility aims to make it easy to generate TPC-DS data, and to run TPC-DS benchmarks against different versions of Spark. The main use case for this utility is to test performance at scale when evaluating changes to Spark or to its underlying infrastructure. The benchmarks run SQL queries against structured datasets. This utility is thus not useful for running tests in streaming workflows.

2. [Sort Benchmark](http://sortbenchmark.org/): This is a single benchmark that sorts a large amount of data generated by the [gensort](http://www.ordinal.com/gensort.html) program.

The benchmark suite can be run on MacOS or CentOS 6+. It does not currently support running on Windows.

# Usage

The benchmark suite requires a storage layer, distributed (such as HDFS/S3/Azure Blob Storage) or local to store the generated test data, as well as the computation
results. This tool also requires a cluster manager, such as YARN for running the Spark driver and executors when running in non-local mode.

1. Run `./gradlew distTar` to build the initial distribution.
2. Get the distribution from `spark-tpcds-benchmark-runner/build/distributions/spark-tpcds-benchmark-runner-<VERSION>.tgz`
3. Upload and unpack the distribution to a node in the cluster.
4. In the distribution, edit `var/conf/config.yml` to match the benchmarking environment you will run with. **Documentation for the various configurable options are described in the [config.yml](https://github.com/palantir/spark-tpcds-benchmark/blob/develop/spark-tpcds-benchmark-runner/var/conf/config.yml) file.**
5. Set the JAVA_HOME environment variable to point to Java 11.
6. Run `service/bin/init.sh start`. The benchmarks will begin running in the background. The driver exits upon
   completing the benchmark suite.

The performance results of running the benchmarks can be found in JSON files located under `benchmark_results/` in the specified metrics filesystem.
You may use a Spark shell to load these JSON files into DataFrames for additional analysis, or download these JSON files
for processing by other tools. Results are grouped by data scale defined by the configuration's `dataScalesGb`, described below.

For TPC-DS, the correctness of the computation is checked against the results of previous executions of the benchmark against the
same set of data. If the source data is regenerated and the previous source data is overwritten, the computation results
from previous runs are also invalidated.
